# Evaluation Usage Guide

This document provides detailed instructions for evaluating agent frameworks in the Agentic AI RAG Benchmark project.

## Overview

The project includes an evaluation script that runs all agent frameworks with the same input, collects their outputs, and evaluates them based on several metrics:

- Factual overlap with the RAG-provided context
- Token usage (estimated)
- Response time
- Reasoning clarity

## Running the Evaluation Script

### Using Python

To run the evaluation script using Python:

```bash
# Activate your virtual environment
source venv/bin/activate

# Run the evaluation script
python -m tests.evaluate_outputs --company "Tesla" --output "results.json"
```

Available command-line options:

- `--company`: The company or topic to research (default: "Tesla")
- `--frameworks`: Comma-separated list of frameworks to evaluate (default: all available frameworks)
- `--output`: Output file path (default: "evaluation_results.json")
- `--format`: Output format, either "json" or "csv" (default: "json")
- `--verbose`: Enable verbose output (default: False)

### Using Docker

To run the evaluation script using Docker:

```bash
# Run the evaluation script
docker-compose run --rm agent_runner python -m tests.evaluate_outputs --company "Tesla" --output "/app/data/results.json"
```

## Understanding Evaluation Metrics

### Factual Overlap

This metric measures how much of the agent's output is factually consistent with the information provided by the RAG service. It is calculated by:

1. Extracting key facts from the RAG-provided context
2. Extracting key facts from the agent's output
3. Calculating the overlap between these two sets of facts

A higher factual overlap score indicates that the agent is effectively using the information provided by the RAG service.

### Token Usage

This metric measures the number of tokens used by the agent during execution. It includes:

- Input tokens: Tokens used in prompts and context
- Output tokens: Tokens generated by the agent
- Total tokens: Sum of input and output tokens

A lower token usage indicates more efficient use of the language model.

### Response Time

This metric measures the time taken by the agent to complete the task. It includes:

- Total time: Total time from start to finish
- Thinking time: Time spent on reasoning and planning
- Execution time: Time spent on executing actions

A lower response time indicates faster execution.

### Reasoning Clarity

This metric evaluates the clarity and coherence of the agent's reasoning process. It is calculated using an LLM-based scoring system that assesses:

- Logical flow: How well the reasoning follows a logical progression
- Explainability: How well the agent explains its decisions
- Coherence: How well the reasoning holds together as a whole

A higher reasoning clarity score indicates clearer and more coherent reasoning.

## Visualizing Evaluation Results

The evaluation script outputs results in either JSON or CSV format, which can be visualized in the UI or using external tools.

### Using the UI

To visualize evaluation results in the UI:

1. Start the UI: `npm start --prefix ui/viewer`
2. Navigate to `http://localhost:3000`
3. Go to the "Evaluation" tab
4. Upload the evaluation results file

The UI provides several visualization options:

- Bar charts comparing metrics across frameworks
- Radar charts showing the strengths and weaknesses of each framework
- Tables with detailed metric values
- Heatmaps highlighting the best-performing frameworks for each metric

### Using External Tools

The evaluation results can also be visualized using external tools:

- **JSON format**: Can be loaded into tools like Jupyter Notebook, Observable, or custom visualization scripts
- **CSV format**: Can be loaded into tools like Excel, Google Sheets, or Tableau

## Customizing Evaluation

### Adding Custom Metrics

To add custom evaluation metrics, modify the `tests/evaluate_outputs.py` file:

```python
def evaluate_custom_metric(agent_output, rag_context):
    # Implement custom metric calculation
    return score

# Add custom metric to the evaluation
metrics["custom_metric"] = evaluate_custom_metric(agent_output, rag_context)
```

### Adjusting Metric Weights

To adjust the weights of different metrics in the overall score, modify the `METRIC_WEIGHTS` dictionary in the `tests/evaluate_outputs.py` file:

```python
METRIC_WEIGHTS = {
    "factual_overlap": 0.4,
    "token_usage": 0.2,
    "response_time": 0.1,
    "reasoning_clarity": 0.3,
    "custom_metric": 0.2
}
```

## Batch Evaluation

For evaluating multiple companies or topics at once, use the batch evaluation functionality:

```bash
# Create a list of companies
echo "Tesla\nApple\nMicrosoft\nGoogle\nAmazon" > companies.txt

# Run batch evaluation
python -m tests.evaluate_outputs --batch companies.txt --output "batch_results.json"
```

The batch results will include separate evaluations for each company, as well as aggregated metrics across all companies.

## Troubleshooting

### Evaluation Script Not Running

If the evaluation script fails to run:

1. Check if all agent frameworks are properly installed
2. Verify that the RAG service is running and accessible
3. Check the logs for error messages

### Inconsistent Results

If the evaluation results are inconsistent:

1. Ensure that the RAG service has the same data for all evaluations
2. Verify that the agent frameworks are using the same configuration
3. Run the evaluation multiple times and average the results

### Memory Issues

If the evaluation script runs out of memory:

1. Reduce the number of frameworks being evaluated at once
2. Reduce the complexity of the task
3. Increase the available memory

## Next Steps

For more information on the evaluation script implementation, see the [Evaluation Script](../architecture/evaluation.md) documentation.
